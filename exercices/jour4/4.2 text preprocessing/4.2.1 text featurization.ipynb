{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%git clone https://github.com/Hotsnown/seminaire-bordeaux-2022.git seminaire &> /dev/null\n",
    "%pip install nbautoeval &> /dev/null\n",
    "from evaluation.jour2.listes.listes import exo_create_list, exo_add_list, exo_lenght, exo_get_item, exo_is_empty, exo_less_than_5, exo_first_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW\n",
    "Count: Convert a collection of text documents to a matrix of token counts. Every sample text is represented as a vector indicating the count of a token in the text.\n",
    "\n",
    "Counting the occurrences of tokens in each document.\n",
    "\n",
    "When using Countvectorizer(), the tokens that occurs very frequently in every document are not penalized.\n",
    "\n",
    "binary=True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english', binary=True)\n",
    "# Fit and transform CountVectorizer to both training and test sets\n",
    "countvec.fit(list(X_train) + list(X_test))\n",
    "X_train_countvec =  countvec.transform(X_train) \n",
    "X_test_countvec = countvec.transform(X_test)\n",
    "target_names = lbl_enc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of the Bag-of-Words Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Collect Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a snippet of the first few lines of text from the book “A Tale of Two Cities” by Charles Dickens, taken from Project Gutenberg.\n",
    "\n",
    "- It was the best of times,\n",
    "- it was the worst of times,\n",
    "- it was the age of wisdom,\n",
    "- it was the age of foolishness,\n",
    "\n",
    "For this small example, let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Design the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a list of all of the words in our model vocabulary.\n",
    "\n",
    "The unique words here (ignoring case and punctuation) are:\n",
    "\n",
    "- “it”\n",
    "- “was”\n",
    "- “the”\n",
    "- “best”\n",
    "- “of”\n",
    "- “times”\n",
    "- “worst”\n",
    "- “age”\n",
    "- “wisdom”\n",
    "- “foolishness”\n",
    "\n",
    "That is a vocabulary of 10 words from a corpus containing 24 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Document Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to score the words in each document.\n",
    "\n",
    "The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.\n",
    "\n",
    "Because we know the vocabulary has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word.\n",
    "\n",
    "The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n",
    "\n",
    "Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“It was the best of times“) and convert it into a binary vector.\n",
    "\n",
    "The scoring of the document would look as follows:\n",
    "\n",
    "- “it” = 1\n",
    "- “was” = 1\n",
    "- “the” = 1\n",
    "- “best” = 1\n",
    "- “of” = 1\n",
    "- “times” = 1\n",
    "- “worst” = 0\n",
    "- “age” = 0\n",
    "- “wisdom” = 0\n",
    "- “foolishness” = 0\n",
    "\n",
    "As a binary vector, this would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other three documents would look as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "#\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "#\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All ordering of the words is nominally discarded and we have a consistent way of extracting features from any document in our corpus, ready for use in modeling.\n",
    "\n",
    "New documents that overlap with the vocabulary of known words, but may contain words outside of the vocabulary, can still be encoded, where only the occurrence of known words are scored and unknown words are ignored.\n",
    "\n",
    "You can see how this might naturally scale to large vocabularies and larger documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exo_count_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexo_count_vectorizer\u001b[49m\u001b[38;5;241m.\u001b[39mexample()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exo_count_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "exo_count_vectorizer.example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize(norm_corpus):\n",
    "    return \"\"\n",
    "\n",
    "exo_count_vectorizer.correction(count_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/_images/text-representation-bow.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab330c63be1774c7d4efd28485dada9f7c6eb3392ec4de9472ccb4da1daed7d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
